{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Learner algorithm (PRISM): Notes\n",
    "\n",
    "Use lecture slides, book chapters or the original paper to understand the PRISM algorithm. \n",
    "\n",
    "Read this notebook. Then, __IN A SEPARATE NOTEBOOK__, implement the algorithm. Implement it step-by-step, add explanations of each step, keep the code clean and modular. Finally, test the correctness of your algorithm on the contact lenses [dataset](contact_lenses.csv) discussed in class, to see if it produces the same rules.\n",
    "\n",
    "If the PRISM algorithm is designed to cover the entire dataset, then the rules can be organized into a decision table, and used for classification. However, in this project, we are more interested in extracting small nuggets of knowledge: the most reliable rules - with significant coverage and high accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Advice: work on the  algorithm implementation in IDE (such as Pycharm). This will allow to fix bugs easily. \n",
    "- After your algorithm works, you can copy it step-by-step to a new notebook and explain each step.\n",
    "- Use `pandas` and `numpy` whenever possible to make the program faster (See an example in Section 4 of this notebook).\n",
    "- Use list comprehensions instead of loops - these run at least 10 times faster than native Python loops.\n",
    "- __Important:__ you do not have to use the starter logic below. You can write your own code from scratch. The goal was to help you, not to constrain your creativity to my way of thinking about the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation notes\n",
    "\n",
    "First, we need a datastructure to store each rule. \n",
    "Possible implementation of required data types is given below. You are free to either reuse it or create your own.\n",
    "\n",
    "Each Rule consists of antecedent (Left Hand Side) and consequent (Right Hand Side). The LHS includes multiple conditions joined with AND, and RHS is a class label. The Rule also needs to store its accuracy and coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rule:\n",
    "    def __init__(self, class_label):\n",
    "        self.conditions = []  # list of conditions\n",
    "        self.class_label = class_label  # rule class\n",
    "        self.accuracy = 0\n",
    "        self.coverage = 0\n",
    "        \n",
    "    def add_condition(self, condition):\n",
    "        self.conditions.append(condition)\n",
    "\n",
    "    def set_params(self, accuracy, coverage):\n",
    "        self.accuracy = accuracy\n",
    "        self.coverage = coverage\n",
    "        \n",
    "    def to_filter(self):\n",
    "        result = \"\"\n",
    "        for cond in self.conditions:\n",
    "            result += cond.to_filter() + \" & \"\n",
    "        result += \"(current_data[columns[-1]] == class_label)\"\n",
    "        return result\n",
    "    \n",
    "    def to_filter_no_class(self):\n",
    "        result = \"\"\n",
    "        for cond in self.conditions:\n",
    "            result += cond.to_filter() + \" & \"\n",
    "        result += \"True\"\n",
    "        return result\n",
    "    \n",
    "    # Human-readable printing of this Rule\n",
    "    def __repr__(self):\n",
    "        return \"If {} then {}. Coverage:{}, accuracy: {}\".format(self.conditions, self.class_label,\n",
    "                                                                 self.coverage, self.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of conditions contains several objects of class _Condition_. \n",
    "\n",
    "Each condition includes the _attribute name_ and the _value_. \n",
    "\n",
    "If the _value_ is numeric, then the condition also includes an additional field `true_false` which means the following: \n",
    "- *if true_false == True then values are >= value* \n",
    "- *if true_false == False then values are < value*\n",
    "- If *true_false is None*, then this condition is simply of form *categorical attribute = value*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Condition:\n",
    "    def __init__(self, attribute, value, true_false = None):\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "        self.true_false = true_false\n",
    "        \n",
    "    def to_filter(self):\n",
    "        result = \"\"\n",
    "        if self is None:\n",
    "            return result\n",
    "        if self.true_false is None:\n",
    "            result += '(current_data[\"' + self.attribute + '\"]' + \"==\" + '\"' + self.value + '\")'\n",
    "        elif self.true_false:\n",
    "            result += '(current_data[\"' + self.attribute + '\"]' + \">=\" + self.value + \")\"\n",
    "        else:\n",
    "            result += '(current_data[\"' + self.attribute + '\"]' + \"<\" + self.value + \")\"\n",
    "        return result\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.true_false is None:\n",
    "            return \"{}={}\".format(self.attribute, self.value)\n",
    "        else:\n",
    "            return \"{}>={}:{}\".format(self.attribute, self.value, self.true_false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes the `learn_one_rule` algorithm. The required parameters are the names of the columns, the current subset of data, and the class label (RHS of the rule we are trying to learn). The optional parameters are thresholds `min_coverage` and `min_accuracy`. I find that it is easier to treat `min_coverage` as an absolute number rather than the proportion of records covered by the rule. For normal datasets we can set this value to 30, for example, to prevent creating unreliable rules with insignificant support from data.\n",
    "\n",
    "The algorithm returns a new rule and maybe a subset of data covered by the Rule or the remaining data (not covered by the rule). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def learn_one_rule(columns, data, class_label, prev_rule=None, min_coverage = 30, min_accuracy = 0.6):\n",
    "    current_data = data.copy()\n",
    "\n",
    "    current_rule = prev_rule\n",
    "    current_accuracy = 0\n",
    "    current_coverage = 0\n",
    "    covered_subset = None\n",
    "    \n",
    "    if current_rule is not None:\n",
    "        current_accuracy = current_rule.accuracy\n",
    "        current_coverage = current_rule.coverage\n",
    "    \n",
    "    best_col = None\n",
    "    best_val = None\n",
    "    true_false = None\n",
    "\n",
    "    for col in columns[:-1]:\n",
    "        \n",
    "        unique_vals = current_data[col].unique().tolist()\n",
    "        \n",
    "        for val in unique_vals:\n",
    "            \n",
    "            rule_filter = np.ones(len(current_data[columns[-1]]), dtype=bool)\n",
    "            if current_rule is not None:\n",
    "                rule_filter = eval(current_rule.to_filter_no_class())\n",
    "            \n",
    "            if isinstance(val, int) or isinstance(val, float):\n",
    "                    \n",
    "                bigger = sum(np.where((current_data[col] >= val) & rule_filter, 1, 0))\n",
    "                smaller = sum(np.where((current_data[col] < val) & rule_filter, 1, 0))\n",
    "                bigger_subset = current_data[(current_data[col] >= val) & rule_filter]\n",
    "                smaller_subset = current_data[(current_data[col] < val) & rule_filter]\n",
    "                \n",
    "                maxi = max(bigger, smaller, min_coverage)\n",
    "                if bigger ==  maxi or  smaller == maxi:\n",
    "                    bigger_cov = sum(np.where(bigger_subset[columns[-1]] == class_label, 1, 0))\n",
    "                    bigger_acc = bigger_cov/sum(bigger_subset[columns[-1]])\n",
    "                    \n",
    "                    smaller_cov = sum(np.where(smaller_subset[columns[-1]] == class_label, 1, 0))\n",
    "                    smaller_acc = smaller_cov/sum(smaller_subset[columns[-1]])\n",
    "                    \n",
    "                    choose_bigger = True if bigger_acc > smaller_acc else False\n",
    "                    if bigger_acc == smaller_acc:\n",
    "                        if bigger_cov > smaller_cov:\n",
    "                            choose_bigger = True\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                \n",
    "                if bigger_acc >= current_accuracy or smaller_acc >= current_accuracy:\n",
    "                    best_col = col\n",
    "                    best_val = val\n",
    "                    current_accuracy = bigger_acc if choose_bigger else smaller_acc\n",
    "                    current_coverage = bigger_cov if choose_bigger else smaller_cov\n",
    "                    true_false = True if choose_bigger else False\n",
    "              \n",
    "            else:\n",
    "                curr_subset = current_data[(current_data[col] == val) & rule_filter]\n",
    "                total = len(curr_subset[columns[-1]])\n",
    "                if total == 0:\n",
    "                    continue\n",
    "\n",
    "                curr_cov = len(curr_subset[curr_subset[columns[-1]] == class_label])\n",
    "                curr_acc = curr_cov/total\n",
    "\n",
    "                if curr_acc >= current_accuracy and curr_cov >= min_coverage:\n",
    "                    best_col = col\n",
    "                    best_val = val\n",
    "                    current_accuracy = curr_acc\n",
    "                    current_coverage = curr_cov\n",
    "                    true_false = None\n",
    "\n",
    "    if best_col is not None:\n",
    "\n",
    "        if current_rule is None:\n",
    "            current_rule = Rule(class_label)\n",
    "\n",
    "        condition = Condition(best_col, best_val, true_false)\n",
    "        current_rule.add_condition(condition)\n",
    "        current_rule.set_params(current_accuracy, current_coverage)\n",
    "        rule_filter = eval(current_rule.to_filter_no_class())\n",
    "\n",
    "        if true_false is None:\n",
    "            covered_subset = current_data[rule_filter]\n",
    "        else:\n",
    "            if true_false == True:\n",
    "                covered_subset = current_data[rule_filter]\n",
    "            else:\n",
    "                covered_subset = current_data[rule_filter]\n",
    "    \n",
    "    return (current_rule, covered_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the main algorithm `learn_rules` takes as parameters list of columns, with the last column representing the class label, and the original data in form of pandas dataframe. Optionally, you should be able to specify the list of classes that you are interested to explore first. Two optional threshold parameters `min_coverage` and `min_accuracy` set up the conditions of rule's validity for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def learn_rules (columns, data, classes=None, \n",
    "                 min_coverage = 30, min_accuracy = 0.6):\n",
    "    rules = []\n",
    "    if classes is not None:\n",
    "        class_labels = classes\n",
    "    else:\n",
    "        class_labels = data[columns[-1]].unique().tolist()\n",
    "\n",
    "    current_data = data.copy()\n",
    "    \n",
    "    for class_label in class_labels:\n",
    "        done = False\n",
    "        while len(current_data) > min_coverage and not done:\n",
    "            # Learn a rule with a single condition\n",
    "            \n",
    "            (rule, current_subset) = learn_one_rule(columns, current_data, class_label, None, min_coverage = 1)\n",
    "            # If the best rule does not pass the coverage threshold - we are done with this class\n",
    "            if rule is None:\n",
    "                break\n",
    "\n",
    "            # If we get the rule with coverage above threshold\n",
    "            # We try to refine this rule\n",
    "            if rule is not None:\n",
    "                # try to improve the rule using the same learn_one_rule and passing existing rule as parameter\n",
    "                # here need another loop which stops when accuracy is not improving\n",
    "                prev_acc=0\n",
    "                new_acc=rule.accuracy\n",
    "                while(new_acc < 1.0 and new_acc > prev_acc):\n",
    "                    (rule, current_subset) = learn_one_rule(columns, current_subset, class_label, rule, min_coverage, min_accuracy)\n",
    "                    prev_acc = new_acc\n",
    "                    new_acc = rule.accuracy\n",
    "                # done with this rule\n",
    "                if rule.accuracy >= min_accuracy:\n",
    "                    rules.append(rule)\n",
    "                    current_data = current_data.drop(current_data[eval(rule.to_filter())].index)\n",
    "                else:\n",
    "                    done = True\n",
    "            else:\n",
    "                done = True\n",
    "                \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correctness test  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your algorithm on the original dataset from the PRISM paper.\n",
    "\n",
    "The dataset was downloaded from [here](https://archive.ics.uci.edu/ml/datasets/Lenses). The CSV version is included in this repository.\n",
    "\n",
    "**Attribute Information**:\n",
    "\n",
    "3 Classes:\n",
    "- __1__ : the patient should be fitted with __hard__ contact lenses,\n",
    "- __2__ : the patient should be fitted with __soft__ contact lenses,\n",
    "- __3__ : the patient should __not__ be fitted with contact lenses.\n",
    "\n",
    "\n",
    "Attributes:\n",
    "1. age of the patient: (1) young, (2) pre-presbyopic, (3) presbyopic\n",
    "2. spectacle prescription:  (1) myope, (2) hypermetrope\n",
    "3. astigmatic:     (1) no, (2) yes\n",
    "4. tear production rate:  (1) reduced, (2) normal\n",
    "\n",
    "Presbyopia is physiological insufficiency of accommodation associated with the aging of the eye that results in progressively worsening ability to focus clearly on close objects. So \"age=presbiopic\" means old.\n",
    "\n",
    "Hypermetropia: far-sightedness, also known as long-sightedness - cannot see close.\n",
    "Myopia: nearsightedness - cannot see at distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'spectacles', 'astigmatism', 'tear production rate',\n",
       "       'lenses type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_file = \"contact_lenses.csv\"\n",
    "data = pd.read_csv(data_file, index_col=['id'])\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace numbers with actual values - for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes\n",
    "conditions = [ data['lenses type'].eq(1), data['lenses type'].eq(2), data['lenses type'].eq(3)]\n",
    "choices = [\"hard\",\"soft\",\"none\"]\n",
    "\n",
    "data['lenses type'] = np.select(conditions, choices)\n",
    "\n",
    "# age groups\n",
    "conditions = [ data['age'].eq(1), data['age'].eq(2), data['age'].eq(3)]\n",
    "choices = [\"young\",\"medium\",\"old\"]\n",
    "\n",
    "data['age'] = np.select(conditions, choices)\n",
    "\n",
    "# spectacles\n",
    "conditions = [ data['spectacles'].eq(1), data['spectacles'].eq(2)]\n",
    "choices = [\"nearsighted\",\"farsighted\"]\n",
    "\n",
    "data['spectacles'] = np.select(conditions, choices)\n",
    "\n",
    "# astigmatism\n",
    "conditions = [ data['astigmatism'].eq(1), data['astigmatism'].eq(2)]\n",
    "choices = [\"no\",\"yes\"]\n",
    "\n",
    "data['astigmatism'] = np.select(conditions, choices)\n",
    "\n",
    "# tear production rate\n",
    "conditions = [ data['tear production rate'].eq(1), data['tear production rate'].eq(2)]\n",
    "choices = [\"reduced\",\"normal\"]\n",
    "\n",
    "data['tear production rate'] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test (do not run it before you finished the implementation of the rule learning algorithm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If [tear production rate=reduced] then none. Coverage:12, accuracy: 1.0\n",
      "If [astigmatism=no, spectacles=farsighted] then soft. Coverage:3, accuracy: 1.0\n",
      "If [astigmatism=no, age=medium] then soft. Coverage:1, accuracy: 1.0\n",
      "If [astigmatism=no, age=young] then soft. Coverage:1, accuracy: 1.0\n",
      "If [age=young] then hard. Coverage:2, accuracy: 1.0\n",
      "If [spectacles=nearsighted, astigmatism=yes] then hard. Coverage:2, accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "column_list = data.columns.to_numpy().tolist()\n",
    "rules = learn_rules (column_list, data, None, 1, 0.95)\n",
    "for rule in rules[:20]:\n",
    "    print(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My results are given below for comparison:\n",
    "\n",
    "If [tear production rate=reduced] then none. Coverage:12, accuracy: 1.0\n",
    "\n",
    "If [astigmatism=no, spectacles=farsighted] then soft. Coverage:3, accuracy: 1.0\n",
    "\n",
    "If [astigmatism=no, age=young] then soft. Coverage:1, accuracy: 1.0\n",
    "\n",
    "If [astigmatism=no, age=medium] then soft. Coverage:1, accuracy: 1.0\n",
    "\n",
    "If [age=young] then hard. Coverage:2, accuracy: 1.0\n",
    "\n",
    "If [spectacles=nearsighted, astigmatism=yes] then hard. Coverage:2, accuracy: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filtering data using conditions\n",
    "\n",
    "Here is an example of how you could use the built-in `pandas` methods to produce a subset covered by a rule.\n",
    "in the same way, you could also filter out rows covered by the rule, to obtain the remaining subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If [astigmatism=no, spectacles=farsighted] then soft. Coverage:3, accuracy: 1\n"
     ]
    }
   ],
   "source": [
    "rule = Rule(\"soft\")\n",
    "cond1 = Condition(\"astigmatism\",\"no\")\n",
    "cond2 = Condition(\"spectacles\",\"farsighted\")\n",
    "rule.add_condition(cond1)\n",
    "rule.add_condition(cond2)\n",
    "rule.set_params(1, 3)\n",
    "print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age  spectacles astigmatism tear production rate lenses type\n",
      "id                                                                 \n",
      "5    young  farsighted          no              reduced        none\n",
      "6    young  farsighted          no               normal        soft\n",
      "13  medium  farsighted          no              reduced        none\n",
      "14  medium  farsighted          no               normal        soft\n",
      "21     old  farsighted          no              reduced        none\n",
      "22     old  farsighted          no               normal        soft\n"
     ]
    }
   ],
   "source": [
    "attr1 = cond1.attribute\n",
    "val1 = cond1.value\n",
    "\n",
    "attr2 = cond2.attribute\n",
    "val2 = cond2.value\n",
    "\n",
    "subset = data[(data[attr1]==val1) & (data[attr2]==val2) ]\n",
    "\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 Marina Barsky. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
